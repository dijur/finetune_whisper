{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai2mP6f_Qmvd"
      },
      "source": [
        "# WhisperTune: Synthetic Data Generation\n",
        "\n",
        "This notebook demonstrates how to generate synthetic speech data for fine-tuning Whisper on low-resource languages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why this Notebook?\n",
        "\n",
        "Fine-tuning Whisper for low-resource languages faces a common challenge: limited high-quality audio-text paired data. This notebook provides a solution by:\n",
        "\n",
        "1. Using Meta's MMS-TTS models to generate synthetic speech\n",
        "2. Applying realistic augmentations to improve robustness\n",
        "3. Creating large-scale training data for low-resource languages\n",
        "\n",
        "The synthetic data can be used to pre-train or supplement existing datasets for Whisper fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Note on Implementation\n",
        "\n",
        "While it's generally good practice to store classes and functions in separate Python modules, this notebook intentionally keeps all code in cells because:\n",
        "\n",
        "1. It's designed for Google Colab usage where direct file imports can be cumbersome\n",
        "2. Makes it easier to share and run without setting up a full development environment\n",
        "3. Allows for interactive experimentation and modification of the code\n",
        "\n",
        "For production use, you may want to refactor the code into proper Python modules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to Use this Notebook\n",
        "\n",
        "### Setup\n",
        "1. Mount your Google Drive using the provided cell\n",
        "2. Import required dependencies (will be installed automatically in Colab)\n",
        "\n",
        "### Configuration\n",
        "1. Modify the `config` dictionary to set:\n",
        "   - TTS model (default: facebook/mms-tts-tgk for Tajik)\n",
        "   - Augmentation parameters\n",
        "   - Output paths\n",
        "\n",
        "### Data Generation\n",
        "1. Prepare your text corpus in a plain text file (one sentence per line)\n",
        "2. Set the `output_dir` and `json_output` paths\n",
        "3. Run the data generation cell with desired parameters:\n",
        "   - `batch_size`: Number of parallel generations\n",
        "   - `sample_size`: How many samples to generate\n",
        "   - `random_seed`: For reproducibility\n",
        "\n",
        "### Output\n",
        "The notebook will generate:\n",
        "- WAV audio files in the specified output directory\n",
        "- A JSONL metadata file with paths and transcripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple\n",
        "from tqdm import tqdm\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from transformers import VitsModel, AutoTokenizer\n",
        "import logging\n",
        "import os\n",
        "import json\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUeXX0qf9WPw",
        "outputId": "41f2061d-bfef-4bfe-e801-169a588b34c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sk-JHcciQxgf"
      },
      "outputs": [],
      "source": [
        "class SyntheticDataGenerator:\n",
        "    def __init__(self, config: Dict):\n",
        "        self.config = config\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self._initialize_model()\n",
        "        self.sampling_rate = self.model.config.sampling_rate\n",
        "\n",
        "    def _initialize_model(self):\n",
        "        \"\"\"Initialize VITS model and tokenizer from HuggingFace.\"\"\"\n",
        "        try:\n",
        "            model_name = self.config['tts'].get('model', 'facebook/mms-tts-tgk')\n",
        "            self.model = VitsModel.from_pretrained(model_name).to(self.device)\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            logger.info(f\"Initialized model {model_name} on {self.device}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error initializing VITS model: {e}\")\n",
        "            raise\n",
        "\n",
        "    def sample_text_corpus(\n",
        "        self,\n",
        "        input_file: str,\n",
        "        sample_size: int,\n",
        "        output_file: str = None,\n",
        "        random_seed: int = None\n",
        "    ) -> List[str]:\n",
        "        \"\"\"\n",
        "        Sample random lines from the input text corpus.\n",
        "\n",
        "        Args:\n",
        "            input_file (str): Path to input text file\n",
        "            sample_size (int): Number of lines to sample\n",
        "            output_file (str): Path to save sampled text (optional)\n",
        "            random_seed (int): Random seed for reproducibility\n",
        "\n",
        "        Returns:\n",
        "            List[str]: Sampled text lines\n",
        "        \"\"\"\n",
        "        if random_seed is not None:\n",
        "            random.seed(random_seed)\n",
        "            np.random.seed(random_seed)\n",
        "\n",
        "        # Read all lines\n",
        "        with open(input_file, 'r', encoding='utf-8') as f:\n",
        "            texts = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "        total_lines = len(texts)\n",
        "        logger.info(f\"Total lines in corpus: {total_lines}\")\n",
        "\n",
        "        if sample_size >= total_lines:\n",
        "            logger.warning(f\"Sample size {sample_size} is >= total lines {total_lines}. Using entire corpus.\")\n",
        "            sampled_texts = texts\n",
        "        else:\n",
        "            # Random sampling without replacement\n",
        "            sampled_indices = random.sample(range(total_lines), sample_size)\n",
        "            sampled_texts = [texts[i] for i in sampled_indices]\n",
        "            logger.info(f\"Sampled {sample_size} lines from corpus\")\n",
        "\n",
        "        # Save sampled text if output file is specified\n",
        "        if output_file:\n",
        "            output_file = str(Path(output_file))\n",
        "            os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "            with open(output_file, 'w', encoding='utf-8') as f:\n",
        "                for text in sampled_texts:\n",
        "                    f.write(text + '\\n')\n",
        "            logger.info(f\"Saved sampled text to {output_file}\")\n",
        "\n",
        "        return sampled_texts\n",
        "\n",
        "    def generate_audio(self, text: str, output_path: str) -> str:\n",
        "        \"\"\"Generate synthetic audio from text.\"\"\"\n",
        "        try:\n",
        "            # Tokenize and generate audio\n",
        "            inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.device)\n",
        "            with torch.no_grad():\n",
        "                waveform = self.model(**inputs).waveform.squeeze()\n",
        "\n",
        "            # Move to CPU and convert to numpy\n",
        "            wav = waveform.cpu().numpy()\n",
        "\n",
        "            # Apply augmentation if enabled\n",
        "            if self.config['augmentation']['enabled']:\n",
        "                wav = self._augment_audio(wav)\n",
        "\n",
        "            # Save audio\n",
        "            sf.write(output_path, wav, self.sampling_rate)\n",
        "            return output_path\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating audio for text: {text}\")\n",
        "            logger.error(f\"Error: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _augment_audio(self, audio: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Apply various augmentation techniques to the audio.\"\"\"\n",
        "        # Add noise if configured\n",
        "        if self.config['augmentation'].get('noise_factor'):\n",
        "            noise = np.random.randn(len(audio))\n",
        "            audio = audio + self.config['augmentation']['noise_factor'] * noise\n",
        "\n",
        "        # Apply speed perturbation\n",
        "        if self.config['augmentation'].get('speed_range'):\n",
        "            speed_factor = np.random.uniform(*self.config['augmentation']['speed_range'])\n",
        "            audio = librosa.effects.time_stretch(audio, rate=speed_factor)\n",
        "\n",
        "        # Apply pitch shift\n",
        "        if self.config['augmentation'].get('pitch_shift_range'):\n",
        "            pitch_shift = np.random.randint(*self.config['augmentation']['pitch_shift_range'])\n",
        "            audio = librosa.effects.pitch_shift(\n",
        "                audio,\n",
        "                sr=self.sampling_rate,\n",
        "                n_steps=pitch_shift\n",
        "            )\n",
        "\n",
        "        return audio\n",
        "\n",
        "    def process_text_corpus(\n",
        "        self,\n",
        "        input_file: str,\n",
        "        output_dir: str,\n",
        "        json_output: str,\n",
        "        batch_size: int = 16,\n",
        "        sample_size: int = None,\n",
        "        random_seed: int = None\n",
        "    ):\n",
        "        \"\"\"Process entire text corpus and generate synthetic audio with metadata.\"\"\"\n",
        "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Sample text corpus if sample_size is specified\n",
        "        if sample_size is not None:\n",
        "            # Create a sampled text file in the same directory as json_output\n",
        "            sampled_text_file = str(Path(json_output).parent / 'sampled_corpus.txt')\n",
        "            texts = self.sample_text_corpus(\n",
        "                input_file=input_file,\n",
        "                sample_size=sample_size,\n",
        "                output_file=sampled_text_file,\n",
        "                random_seed=random_seed\n",
        "            )\n",
        "        else:\n",
        "            # Read entire corpus\n",
        "            with open(input_file, 'r', encoding='utf-8') as f:\n",
        "                texts = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "        logger.info(f\"Processing {len(texts)} text entries...\")\n",
        "\n",
        "        metadata = []\n",
        "        # Process in batches for efficiency\n",
        "        for batch_idx in tqdm(range(0, len(texts), batch_size), desc=\"Generating Audio\"):\n",
        "            batch_texts = texts[batch_idx:batch_idx + batch_size]\n",
        "\n",
        "            # Tokenize batch\n",
        "            inputs = self.tokenizer(batch_texts, return_tensors=\"pt\", padding=True).to(self.device)\n",
        "\n",
        "            # Generate audio for batch\n",
        "            with torch.no_grad():\n",
        "                waveforms = self.model(**inputs).waveform  # [batch, time]\n",
        "\n",
        "            # Process each item in batch\n",
        "            for idx, (text, waveform) in enumerate(zip(batch_texts, waveforms)):\n",
        "                try:\n",
        "                    output_path = os.path.join(output_dir, f\"synthetic_{batch_idx + idx:06d}.wav\")\n",
        "\n",
        "                    # Move to CPU and convert to numpy\n",
        "                    wav = waveform.cpu().numpy()\n",
        "\n",
        "                    # Apply augmentation if enabled\n",
        "                    if self.config['augmentation']['enabled']:\n",
        "                        wav = self._augment_audio(wav)\n",
        "\n",
        "                    # Save audio\n",
        "                    sf.write(output_path, wav, self.sampling_rate)\n",
        "\n",
        "                    metadata.append({\n",
        "                        \"audio_path\": output_path,\n",
        "                        \"text\": text,\n",
        "                        \"duration\": len(wav) / self.sampling_rate\n",
        "                    })\n",
        "\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error processing text entry {batch_idx + idx}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        # Save metadata\n",
        "        with open(json_output, 'w', encoding='utf-8') as f:\n",
        "            for entry in metadata:\n",
        "                f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
        "\n",
        "        logger.info(f\"Generated {len(metadata)} audio files with metadata at {json_output}\")\n",
        "        return metadata\n",
        "\n",
        "    def evaluate_audio_quality(self, audio_path: str) -> Dict[str, float]:\n",
        "        \"\"\"Evaluate the quality metrics of generated audio.\"\"\"\n",
        "        try:\n",
        "            audio, sr = librosa.load(audio_path, sr=self.sampling_rate)\n",
        "            metrics = {}\n",
        "\n",
        "            # Signal-to-noise ratio\n",
        "            noise_floor = np.mean(np.abs(audio[audio < np.mean(audio)]))\n",
        "            signal = np.mean(np.abs(audio[audio >= np.mean(audio)]))\n",
        "            metrics['snr'] = 20 * np.log10(signal / noise_floor) if noise_floor > 0 else float('inf')\n",
        "\n",
        "            # RMS energy\n",
        "            metrics['rms'] = float(np.sqrt(np.mean(audio**2)))\n",
        "\n",
        "            # Zero-crossing rate\n",
        "            metrics['zcr'] = float(np.mean(librosa.feature.zero_crossing_rate(audio)))\n",
        "\n",
        "            # Spectral centroid\n",
        "            cent = librosa.feature.spectral_centroid(y=audio, sr=sr)\n",
        "            metrics['spectral_centroid'] = float(np.mean(cent))\n",
        "\n",
        "            return metrics\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error calculating audio quality metrics: {e}\")\n",
        "            return {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0ZV3nAOQmvg"
      },
      "source": [
        "## Configuration\n",
        "\n",
        "Set up the configuration for synthetic data generation including TTS model, augmentation parameters, and output paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCkWjPoc-Rf4"
      },
      "outputs": [],
      "source": [
        "output_dir = '/content/drive/My Drive/Colab Outputs/audio'\n",
        "json_output = '/content/drive/My Drive/Colab Outputs/metadata.jsonl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jY-n2Be8Qmvg",
        "outputId": "dddb33e9-5250-4cee-a7aa-44aa182efd50"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating Audio: 100%|██████████| 2000/2000 [1:57:03<00:00,  3.51s/it]\n"
          ]
        }
      ],
      "source": [
        "config = {\n",
        "    'tts': {\n",
        "        'model': 'facebook/mms-tts-tgk',\n",
        "        'language': 'tgk'\n",
        "    },\n",
        "    'data': {\n",
        "        'sample_rate': None\n",
        "    },\n",
        "    'augmentation': {\n",
        "        'enabled': True,\n",
        "        'noise_factor': 0.003,\n",
        "        'speed_range': (0.9, 1.1),\n",
        "        'pitch_shift_range': (-2, 2)\n",
        "    }\n",
        "}\n",
        "\n",
        "generator = SyntheticDataGenerator(config)\n",
        "\n",
        "metadata = generator.process_text_corpus(\n",
        "    input_file='final_filtered_newest.txt',\n",
        "    output_dir=output_dir,\n",
        "    json_output=json_output,\n",
        "    batch_size=16,\n",
        "    sample_size=32000,\n",
        "    random_seed=42\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "whisper-tune-GeidTQlu-py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
